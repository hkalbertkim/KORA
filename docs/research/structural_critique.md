# Why Existing LLM Systems Are Inference-Reflexive

LLM APIs are designed for direct prompt-response interaction.

This encourages:

- Immediate inference
- Lack of pre-processing
- No explicit execution graph

This is not a flaw in models.
It is a structural property of how they are used.

KORA introduces execution structure as a missing layer.
